Wie versprochen, widmet sich das hiesige Kapitel mit der Wiederholung und Einf"uhrung wichtigen Vokabulars, welches im Rahmen dieser Arbeit h"aufig zum Einsatz kommen wird.
Von zentraler Bedeutung werden dabei Inhalte der numerischen linearen Algebra sein.\\

Um es nicht bei einer blo"sen Auflistung von Definitionen zu belassen, werden zudem Resultate pr"asentiert, auf die wir sp"ater zur"uckgreifen werden.
Obschon sich der Autor bem"uht hat,
in der Literatur g"angige Notation zu benutzen, bittet er den
verst"andnisvollen Leser bei Unklarheiten im Anhang \ref{appNotation} nachzuschlagen.\\

Mit den Buchstaben $m$ und $n$ werden wir -- sofern nicht anders vermerkt -- zwei nat"urliche Zahlen bezeichnen. Dabei wollen wir die Null aus den nat"urlichen Zahlen $\N$ ausgeschlossen wissen.
Falls die Null zugelassen ist, schreiben wir explizit $\N_0 := \N\cup\{0\}$.
Des Weiteren bezeichnen wir wie "ublich mit $\R$ die Menge der reellen Zahlen, mit $\C$ die Menge der komplexen Zahlen und entsprechend mit $\R^{m,n}$ und $\C^{m,n}$ die Mengen der reell- beziehungsweise komplexwertigen Matrizen. Wir unterschlagen $n$ im Fall $n=1$.\\

In Anlehnung an die in \emph{MATLAB} verwendete Syntax, werden wir Gebrauch von der Notation $i=m:n$ an Stelle von $i=m,m+1,\ldots,n-1,n$ machen.
Diese Kurzschreibweise wird etwa bei der Einf"uhrung von Indizierungen zum Einsatz kommen.
So l"asst sich beispielsweise eine Menge $\{x_1,x_2,\ldots,x_n\}$ mit hier nicht n"aher bestimmten Elementen kurz durch $\{x_k\}_{k=1:n}$ ausdr"ucken.\\

Es sei nun $A$ eine quadratische, komplexwertige Matrix, also $A\in\Cnn$. Diese wird als \emph{hermitesch} bezeichnet, falls sie die Identit"at $A=A^H$ erf"ullt und ist \emph{positiv definit}, sofern
f"ur alle Vektoren $x\in\Co$ die Absch"atzung
\[
x^H A x > 0
\]
gilt. Folglich werden wir eine Matrix \emph{hermitesch positiv definit} (HPD)
nennen, wenn sie sowohl hermitesch als auch positiv definit ist.\\

Ist $A$ solch eine HPD-Matrix,
dann nennen wir eine Menge von Vektoren $\{x_k\}_{k=1:m}\subseteq\Cn$ \emph{orthonormal
bez"uglich} $A$ oder schlicht: \emph{$A$-orthonormal}, falls
\[
x_i^H A x_j = \delta_{i,j} := \begin{cases}
1 & \text{ wenn } i=j \\
0 & \text{ wenn } i\neq j
\end{cases}
\]
f"ur alle $i,j = 1:m$ gilt. Allgemeiner hei"st eine Matrix $X\in\Cnn$ \emph{orthogonal bez"uglich} $A$ oder \emph{$A$-orthogonal}, falls sie
\[
X^H A X = I_n := [\delta_{i,j}]_{i,j=1:n}
\]
erf"ullt.\footnote{Wir werden Matrizen in ihrer komponentenweisen Darstellung konsequent durch eckige Klammern \glqq$[$\grqq\ und \glqq$]$\grqq\ beranden.} Aus der Hermitizit"at von $A$ und $I_n$ folgt nat"urlich ebenfalls $XAX^H = I_n$. F"ur den Fall $A=I_n$ ignorieren wir in der Formulierung den Bezug zu $A$ und sprechen lediglich von Orthogonalit"at beziehungsweise Orthonormalit"at.\\

Sei im Folgenden $A$ wieder eine beliebige komplexwertige Matrix. Neben dieser betrachten wir nun noch eine weitere Matrix $B\in\Cnn$.
Unter einem \emph{$n$-dimensionalen Eigenwertproblem} wollen wir die Aufgabe verstehen, Paare $(\lambda,x)\in\C\times\Co$ zu finden, die der Gleichung
\begin{equation}\label{eq:chap1:evp}
Ax = \lambda Bx
\end{equation}
gen"ugen. Wenn klar ist, dass von solch einem Problem die Rede ist, werden wir anstelle des Wortes \glqq Eigenwertproblem\grqq\ die Notation $(A,B)$ als Bezeichnung verwenden und aus"serdem auf die Angabe der Dimension verzichten. Im Spezialfall $B=I_n$
hei"st $(A,B)$ \emph{gew"ohnliches Eigenwertproblem der Dimension $n$}.\\

Ist ein passendes Paar $(\lambda,x)$ gefunden, welches die \emph{Eigenwertgleichung} \eqref{eq:chap1:evp} l"ost, so nennen wir dieses \emph{Eigenpaar von} $(A,B)$ oder kurz: \emph{Eigenpaar}, falls bekannt ist, von welchem Eigenwertproblem die Rede ist.
Dabei hei"st $\lambda$ \emph{Eigenwert von} $(A,B)$ und $x$ \emph{Eigenvektor zum Eigenwert} $\lambda$ \emph{von} $(A,B)$. Auch hier werden wir auf die Angabe des Eigenwertproblems verzichten, wenn der Kontext dies gestattet.
Beim gew"ohnlichen Eigenwertproblem sehen wir von der Paar-Schreibweise ab und sprechen direkt von Eigenpaaren, Eigenwerten und Eigenvektoren von $A$.\\

Wie auf der Titelseite angek"undigt, wird sich diese Arbeit "uberwiegend mit der Behandlung \emph{hermitesch positiv definiter Eigenwertprobleme} (HPD-Eigenwertprobleme) befassen.
Damit seien f"urderhin Eigenwertprobleme $(A,B)$ bezeichnet, bei denen wir uns mit einer hermiteschen Matrix $A$ sowie einer HPD-Matrix $B$ konfrontiert sehen.\\

Eigenwertprobleme dieser Art besitzen eine Reihe n"utzlicher Eigenschaften, an denen wir uns zu einem sp"ateren Zeitpunkt in der Arbeit bedienen werden.
Von besonderer Bedeutung wird dabei das folgende Resultat sein.

\begin{thm}\label{thm:chap1:realEigenvalues}
Ist $(A,B)$ ein HPD-Eigenwertproblem der Dimension $n$, so sind alle zugeh"origen Eigenwerte reell.
Au"serdem existiert eine $B$-orthogonale Matrix $X\in\Cnn$ sowie eine Diagonalmatrix $\Lambda\in\R^{n,n}$ mit
\begin{equation}\label{eq:chap1:evpMatrix}
A = BX\Lambda X^{-1}.
\end{equation}
\end{thm}

\begin{proof}
Sei $(\lambda,x)$ ein Eigenpaar von $(A,B)$. Der positiven Definitheit von $B$ wegen, gilt nach Definition $x^H B x > 0$. Aus
\[
\lambda (x^H B x) = x^H Ax = x^H A^H x = (Ax)^H x
= \overline{\lambda} (Bx)^H x = \overline{\lambda} (x^H B x)
\]
folgt dann die Gleichheit $\lambda = \overline{\lambda}$ und somit $\lambda\in\R$.\\

 Um die Existenz der im Satz angegebenen Faktorisierung \eqref{eq:chap1:evpMatrix}
zu zeigen, ziehen wir unterst"utzend den Beweis aus ~\cite[Theorem 15.3.2, 344 f.]{parlett} zurate und erg"anzen diesen zur Vervollst"andigung durch weitere Argumente.\\


Zun"achst nutzen wir die Hermitizit"at und die positive Definitheit von $B$ aus. Mit diesen beiden Eigenschaften garantieren uns der Spektralsatz f"ur hermitesche Matrizen\footnote{Eine Formulierung mit Beweis ist im Anhang zu finden. Siehe Satz \ref{thm:appTheorems:Spektralsatz}.}
und der Satz von der Existenz der Cholesky-Zerlegung\footnote{Siehe Satz \ref{thm:appTheorems:Cholesky} im Anhang.} eine Faktorisierung der Art
\begin{equation}\label{eq:chap1:factorization}
B = X_B \Lambda_B^2 X_B^H.
\end{equation}
Dabei ist $\Lambda_B\in\R^{n,n}$ eine diagonale Matrix und $X_B \in \C^{n,n}$ orthogonal.
Wir definieren uns nun eine weitere Matrix
\[
M:= \Lambda_B^{-1} X_B^H A X_B \Lambda_B^{-1},
\]
welche wegen
\[
M^H = \left(\Lambda_B^{-1} X_B^H A X_B \Lambda_B^{-1}\right)^H
= (\Lambda_B^{-1})^{H} X_B^H A^H (X_B^H)^H (\lambda_B^{-1})^H
= \Lambda_B^{-1} X_B^H A X_B \Lambda_B^{-1}
= M
\]
hermitesch ist. Erneut finden wir mit Hilfe des Spektralsatzes eine Zerlegung der Form
\[
M = X_M \Lambda_M X_M
\]
von $M$. Setzen wir nun $X:=X_M \Lambda_M X_M^H$, erhalten wir die Identit"aten
\begin{align*}
X^H A X &= X_M^H \Lambda_B^{-1} X_B^H A X_B \Lambda_B^{-1} X_M = X_M^H M X_M = \Lambda_M\\
X^H B X &= X_M^H \Lambda_B^{-1} X_B^H B X_B \Lambda_B^{-1} X_M \overset{\eqref{eq:chap1:factorization}}{=} X_M^H X_M = I_n.
\end{align*}
Also ist $X$ schon $B$-orthogonal. Schlie"slich folgt Gleichung \eqref{eq:chap1:evpMatrix} aus
\[
X^{-1}B^{-1}AX = X^{-1}B^{-1} (X^H)^{-1} X^H AX = \Lambda_M.
\]
durch Umstellen.
\end{proof}

Die Existenz der Faktorisierung \eqref{eq:chap1:factorization}
werden wir sp"ater nutzen, um bei hermitesch positiv definiten Eigenwertproblem $(A,B)$ anstelle von \eqref{eq:chap1:evp} das L"osen von
\[
AX = BX\Lambda
\]
verlangen zu k"onnen. Diese Notation wird h"aufig bei der Ausformulierung von Algorithmen in Erscheinung treten.\\

Fahren wir fort mit einer beliebig gew"ahlten Matrix $A\in\C^{m,n}$. Diese induziert bekannterweise eine lineare Abbildung $\mathcal{A}\colon\Cn\to\C^{m}$ verm"oge $\mathcal{A}(x) := Ax$.
Wir werden fortan nicht mehr zwischen der Abbildung $\mathcal{A}$ und der zugeh"origen Matrix unterscheiden und wollen $A$ sowohl als Element des Vektorraumes $\C^{m,n}$
als auch als lineare Abbildung zwischen $\C^m$ und $\C^n$ begreifen. Demnach sind Notationen wie $\Bild(A)$ und "ahnliche abbildungsrelevante Schreibweisen wohldefiniert.\\

Nehmen wir uns zu guter Letzt einen $m$-dimensionalen Unterraum $\U \subseteq \C^n$ her und bem"uhen erneut eine Matrix $A\in\Cnn$.
Der eben eingef"uhrte Unterraum hei"st \emph{$A$-invariant}, falls die Aussage $A\U\subseteq\U$ gilt.
Ist dann $U\in\C^{n,m}$ eine Matrix mit $\Bild(U)=\U$, so existiert folglich eine Matrix $V\in\C^{m,m}$ mit $AU = UV$.\\

Ausgestattet mit den eben besprochenen Definitionen und Resulaten, k"onnen wir nun mit dem Hauptteil dieser Arbeit beginnen.



%\begin{prop}
%Ist $p$ eine Projektion auf den Unterraum $U$, dann gilt $p(u) = u$ f"ur
%alle $u \in U$.
%\end{prop}
%\begin{proof}
%Sei zun"achst $u \in \Bild{p}$. Dann existiert $v \in V$ mit $p(v) - u = 0$.
%Nach Definition gilt aber auch $p^2(v)-u = p(u)-u = 0$. Also folgt $p(u)=u$.
%\textcolor{red}{gilt $\Bild(p) = U$?}
%\end{proof}

%Bilinearit"at von $\langle \cdot, \cdot \rangle_A$
%Kontur (komplex), Jordankurven, Invarianter Unterraum, Matrixpolynom\\




%Schur-Vektoren $A = X\Lambda X^H$ sind die $x_i$.
