Wie versprochen, widmet sich das hiesige Kapitel mit der Wiederholung und Einf"uhrung wichtigen Vokabulars, welches im Rahmen dieser Arbeit h"aufig zum Einsatz kommen wird.
Von zentraler Bedeutung werden dabei Inhalte der linearen Algebra sein.\\

Um es nicht bei einer blo"sen Auflistung von Definitionen zu belassen, werden zudem Resultate pr"asentiert, auf die wir sp"ater zur"uckgreifen werden.
Obschon sich der Autor bem"uht hat,
in der Literatur g"angige Notation zu benutzen, bittet er den
verst"andnisvollen Leser bei Unklarheiten im Anhang \ref{appNotation} nachzuschlagen.\\

Mit den Buchstaben $m$ und $n$ werden wir -- sofern nicht anders vermerkt -- zwei nat"urliche Zahlen bezeichnen. Dabei wollen wir die Null aus den nat"urlichen Zahlen $\N$ ausgeschlossen wissen.
Falls die Null zugelassen ist, schreiben wir explizit $\N_0 := \N\cup\{0\}$.
Des Weiteren bezeichnen wir mit $\R$ und $\C$ wie "ublich die Mengen der reellen respektive komplexen Zahlen und entsprechend mit $\R^{m,n}$ und $\C^{m,n}$ die Mengen der reell- beziehungsweise komplexwertigen $(m\times n)$-Matrizen. Wir unterschlagen $n$ im Fall $n=1$.\\

In Anlehnung an die in \textsc{matlab} verwendete Syntax werden wir Gebrauch von der Notation $i=m:n$ an Stelle von $i=m,m+1,\ldots,n-1,n$ machen.
Diese Kurzschreibweise wird etwa bei der Einf"uhrung von Indizierungen zum Einsatz kommen.
So l"asst sich beispielsweise eine Menge $\{x_1,x_2,\ldots,x_n\}$ mit hier nicht n"aher bestimmten Elementen kurz durch $\{x_k\}_{k=1:n}$ ausdr"ucken.\\

Es sei nun $A$ eine quadratische, komplexwertige Matrix, also $A\in\Cnn$. Diese wird als \emph{hermitesch} bezeichnet, falls sie die Identit"at $A=A^H$ erf"ullt und ist \emph{positiv definit}, sofern
f"ur alle Vektoren $x\in\Co$ die Absch"atzung
\[
x^H A x > 0
\]
gilt. Folglich werden wir eine Matrix \emph{hermitesch positiv definit} (HPD)
nennen, wenn sie sowohl hermitesch als auch positiv definit ist.\\

Ist $A$ solch eine HPD-Matrix,
dann nennen wir eine Menge von Vektoren $\{x_k\}_{k=1:m}\subseteq\Cn$ \emph{orthonormal
bez"uglich} $A$ oder schlicht: \emph{$A$-orthonormal}, falls
\[
x_i^H A x_j = \delta_{i,j} := \begin{cases}
1 & \text{ wenn } i=j \\
0 & \text{ wenn } i\neq j
\end{cases}
\]
f"ur alle $i,j = 1:m$ gilt. Allgemeiner hei"st eine Matrix $X\in\Cnn$ \emph{orthogonal bez"uglich} $A$ oder \emph{$A$-orthogonal}, falls sie
\[
X^H A X = I_n := [\delta_{i,j}]_{i,j=1:n}
\]
erf"ullt.\footnote{Wir werden Matrizen in ihrer komponentenweisen Darstellung konsequent durch eckige Klammern, also durch \glqq$[$\grqq\ und \glqq$]$\grqq, beranden.} Aus der Hermitizit"at von $A$ und $I_n$ folgt nat"urlich ebenfalls $XAX^H = I_n$. F"ur den Fall $A=I_n$ ignorieren wir in der Formulierung den Bezug zu $A$ und sprechen lediglich von Orthogonalit"at beziehungsweise Orthonormalit"at.\footnote{Streng genommen sollte hier zwischen \emph{orthogonal} und \emph{unit"ar} unterschieden werden. Wir werden jedoch im Rahmen dieser Arbeit auf diese Unterscheidung verzichten und die Worte "aquivalent gebrauchen.}\\

Sei im Folgenden $A$ wieder eine beliebige komplexwertige Matrix. Neben dieser betrachten wir nun noch eine weitere Matrix $B\in\Cnn$.
Unter einem \emph{$n$-dimensionalen Eigenwertproblem} wollen wir die Aufgabe verstehen, Paare $(\lambda,x)\in\C\times(\Co)$ zu finden, die der Gleichung
\begin{equation}\label{eq:chap2:evp}
Ax = \lambda Bx
\end{equation}
gen"ugen. Wenn klar ist, dass von solch einem Problem die Rede ist, werden wir anstelle des Wortes \glqq Eigenwertproblem\grqq\ die Notation $(A,B)$ als Bezeichnung verwenden und au"serdem auf die Angabe der Dimension verzichten. Den Spezialfall $(A,I_n)$ bezeichnen wir als \emph{gew"ohnliches Eigenwertproblem}.\\

Ist ein passendes Paar $(\lambda,x)$ gefunden, welches die \emph{Eigenwertgleichung} \eqref{eq:chap2:evp} l"ost, so nennen wir dieses \emph{Eigenpaar von} $(A,B)$ oder kurz: \emph{Eigenpaar}, falls klar ist, von welchem Eigenwertproblem die Rede ist.
Dabei hei"st $\lambda$ \emph{Eigenwert von} $(A,B)$ und $x$ \emph{Eigenvektor zum Eigenwert} $\lambda$ \emph{von} $(A,B)$. Auch hier werden wir auf die Angabe des Eigenwertproblems verzichten, wenn der Kontext dies gestattet.
Beim gew"ohnlichen Eigenwertproblem sehen wir von der Paar-Schreibweise ab und sprechen direkt von Eigenpaaren, Eigenwerten und Eigenvektoren von $A$.
Die Menge aller Eigenwerte von $(A,B)$, das sogenannte \emph{Spektrum}, bezeichnen wir mit $\Lambda(A,B)$ im allgemeinen und mit $\Lambda(A)$ im gew"ohnlichen Fall.\\

Wie auf der Titelseite angek"undigt, wird sich diese Arbeit "uberwiegend mit der Behandlung \emph{hermitesch positiv definiter Eigenwertprobleme} (HPD-Eigenwertprobleme) befassen.
Damit seien f"urderhin Eigenwertprobleme $(A,B)$ bezeichnet, bei denen wir uns mit einer hermiteschen Matrix $A$ sowie einer HPD-Matrix $B$ konfrontiert sehen.

\newpage
Eigenwertprobleme dieser Art besitzen eine Reihe n"utzlicher Eigenschaften, an denen wir uns zu einem sp"ateren Zeitpunkt in der Arbeit bedienen werden.
Von besonderer Bedeutung wird dabei das folgende Resultat sein.

\begin{thm}\label{thm:chap2:realEigenvalues}
Ist $(A,B)$ ein HPD-Eigenwertproblem der Dimension $n$, so sind alle zugeh"origen Eigenwerte reell.
Au"serdem existiert eine $B$-orthogonale Matrix $X\in\Cnn$ sowie eine Diagonalmatrix $\Lambda\in\R^{n,n}$ mit
\begin{equation}\label{eq:chap2:evpMatrix}
A = BX\Lambda X^{-1}.
\end{equation}
\end{thm}

\begin{proof}
Sei $(\lambda,x)$ ein Eigenpaar von $(A,B)$. Der positiven Definitheit von $B$ wegen gilt dann $x^H B x > 0$. Aus
\[
\lambda (x^H B x) = x^H Ax = x^H A^H x = (Ax)^H x
= \overline{\lambda} (Bx)^H x = \overline{\lambda} (x^H B x)
\]
folgt daher die Gleichheit $\lambda = \overline{\lambda}$ und somit $\lambda\in\R$.
 Um die Existenz der im Satz angegebenen Faktorisierung \eqref{eq:chap2:evpMatrix}
zu zeigen, ziehen wir den Beweis aus \cite[Theorem 15.3.2]{parlett} unterst"utzend zurate und erg"anzen diesen durch weitere Argumente.\\

Zun"achst nutzen wir die Hermitizit"at und die positive Definitheit von $B$ aus: Mit diesen beiden Eigenschaften garantieren uns der Spektralsatz f"ur hermitesche Matrizen\footnote{
Siehe Satz \ref{thm:appTheorems:Spektralsatz} im Anhang \ref{appTheorems}.}
und der Satz von der Existenz der Cholesky-Zerlegung\footnote{Siehe Satz \ref{thm:appTheorems:Cholesky} im Anhang \ref{appTheorems}.} die Faktorisierung
\begin{equation}\label{eq:chap2:factorization}
B = X_B \Lambda_B^2 X_B^H.
\end{equation}
Dabei ist $\Lambda_B\in\R^{n,n}$ eine diagonale Matrix mit echt positiven Eintr"agen und $X_B \in \C^{n,n}$ orthogonal.
Wir definieren nun eine weitere Matrix
$M:= \Lambda_B^{-1} X_B^H A X_B \Lambda_B^{-1}$,
welche wegen
\[
M^H = \left(\Lambda_B^{-1} X_B^H A X_B \Lambda_B^{-1}\right)^H
= (\Lambda_B^{-1})^{H} X_B^H A^H (X_B^H)^H (\lambda_B^{-1})^H
= \Lambda_B^{-1} X_B^H A X_B \Lambda_B^{-1}
= M
\]
hermitesch ist. Nutzen wir erneut den Spektralsatzes, so finden wir eine orthogonale Matrix $X_M\in\Cnn$ und eine Diagonalmatrix $\Lambda_M\in\R^{n,n}$, sodass
\[
M = X_M \Lambda_M X_M^H
\]
gilt. Setzen wir nun $X:=X_B \Lambda_B^{-1} X_M$, erhalten wir die Identit"aten
\begin{align*}
X^H A X &= X_M^H \Lambda_B^{-1} X_B^H A X_B \Lambda_B^{-1} X_M = X_M^H M X_M = \Lambda_M\\
X^H B X &= X_M^H \Lambda_B^{-1} X_B^H B X_B \Lambda_B^{-1} X_M \overset{\eqref{eq:chap2:factorization}}{=} X_M^H X_M = I_n.
\end{align*}
Also ist $X$ schon $B$-orthogonal. Schlie"slich folgt Gleichung \eqref{eq:chap2:evpMatrix} aus
\[
X^{-1}B^{-1}AX = X^{-1}B^{-1} (X^H)^{-1} X^H AX = \Lambda_M.
\]
durch Umstellen.
\end{proof}

Die Existenz der Faktorisierung \eqref{eq:chap2:evpMatrix}
werden wir sp"ater nutzen, um bei hermitesch positiv definiten Eigenwertproblem $(A,B)$ anstelle von \eqref{eq:chap2:evp} das L"osen von
\begin{equation}\label{eq:chap2:evpMatrix2}
AX = BX\Lambda
\end{equation}
verlangen zu k"onnen. Diese Notation wird h"aufig bei der Ausformulierung von Algorithmen in Erscheinung treten.\\

Fahren wir fort mit einer beliebig gew"ahlten Matrix $A\in\C^{m,n}$. Diese induziert bekannterweise eine lineare Abbildung $\mathcal{A}\colon\Cn\to\C^{m}$ verm"oge $\mathcal{A}(x) := Ax$.
Wir werden fortan nicht mehr zwischen der Abbildung $\mathcal{A}$ und der zugeh"origen Matrix unterscheiden und wollen $A$ sowohl als Element des Vektorraumes $\C^{m,n}$
als auch als lineare Abbildung von $\C^n$ nach $\C^m$ begreifen. Demnach sind Notationen wie $\Bild(A)$ und "ahnliche abbildungsrelevante Schreibweisen wohldefiniert.\\

Nehmen wir uns zu guter Letzt einen $m$-dimensionalen Unterraum $\U \subseteq \C^n$ her und bem"uhen erneut eine Matrix $A\in\Cnn$.
Der eben eingef"uhrte Unterraum hei"st \emph{$A$-invariant}, falls die Inklusion $A\U\subseteq\U$ gilt.
Ist dann $U\in\C^{n,m}$ eine Matrix mit $\Bild(U)=\U$, so existiert folglich eine Matrix $V\in\C^{m,m}$ mit $AU = UV$.\\

Ausgestattet mit den eben besprochenen Definitionen und Resulaten k"onnen wir nun mit dem Hauptteil dieser Arbeit beginnen.

\newpage
\textcolor{white}{blind}
%\begin{prop}
%Ist $p$ eine Projektion auf den Unterraum $U$, dann gilt $p(u) = u$ f"ur
%alle $u \in U$.
%\end{prop}
%\begin{proof}
%Sei zun"achst $u \in \Bild{p}$. Dann existiert $v \in V$ mit $p(v) - u = 0$.
%Nach Definition gilt aber auch $p^2(v)-u = p(u)-u = 0$. Also folgt $p(u)=u$.
%\textcolor{red}{gilt $\Bild(p) = U$?}
%\end{proof}

%Bilinearit"at von $\langle \cdot, \cdot \rangle_A$
%Kontur (komplex), Jordankurven, Invarianter Unterraum, Matrixpolynom\\




%Schur-Vektoren $A = X\Lambda X^H$ sind die $x_i$.
